---
title: "Lecture 2.2: Comparing Orders of Magnitude"
format:
    revealjs:
        incremental: true
        slideNumber: true
        smaller: true
        center: true
        scrollable: true
---

## Orders of magnitude

- When comparing $t(n)$, focus on orders of magnitude
  - Ignore constant factors
- $f(n)=n^3$ eventually dominates $g(n)=5000n^2$
- How do we compare functions with respect to orders of magnitude?
  
## Upper bounds

- $f(x)$ is said to be $O(g(x))$ if we can find constants $c$ and $x_0$ such that $c \cdot g(x)$ is an upper bound for $f(x)$ for $x$ beyond $x_0$
- $f(x) \leq cg(x)$ for every $x \geq x_0$

## Examples

- $100n+5$ is $O(n^2)$
  - $100n+5 \leq 100n+n=101n$, for $n \geq 5$
  - $101n \leq 101n^2$
  - Choose $n_0=5$ and $c=101$
- Alternatively
  - $100n+5 \leq 100n +5n =105n$, for $n \geq 1$
  - $105n \leq 105n^2$
  - Choose $n_0=1$ and $c=105$
- Choice of $c$ and $n_0$ is not unique

## Examples...

- $100n^2 + 20n +5$ is $O(n^2)$
  - $100n^2 + 20n +5 \leq 100n^2 + 20n^2 +5n^2$, for $n \geq 1$
  - $100n^2 + 20n^2 +5n^2 \leq 125n^2$, for $n \geq 1$
  - Choose $n_0=1$ and $c=125$
- What matters is the highest term
  - $20n+5$ is dominated by $100n^2$
- $n^3$ is not $O(n^2)$
  - No matter what $c$ we choose, $cn^2$ will be dominated by $n^3$ for $n \geq c$

## Useful properties

:::: {.columns}

::: {.column width=50%}

- If $f_{1}(n)$ is $O(g_{1}(n))$ and $f_{2}(n)$ is $O(g_{2}(n))$, then $f_{1}(n)+f_{2}(n)$ is $O(\max(g_{1}(n),g_{2}(n)))$
- Proof
  - $f_{1}(n) \leq c_{1}g_{1}(n)$ for $n \geq n_{1}$
  - $f_{2}(n) \leq c_{2}g_{2}(n)$ for $n \geq n_{2}$
  - Let $c_{3} = \max(c_{1},c_{2})$, $n_{3} = \max(n_{1},n_{2})$
  - For $n \geq n_{3}$, $f_{1}(n)+f_{2}(n)$
    - $\leq c_{1}g_{1}(n)+c_{2}g_{2}(n)$
    - $\leq c_{3}g_{1}(n)+c_{3}g_{2}(n)$
    - $\leq 2c_{3}(\max(g_{1}(n),g_{2}(n)))$

:::

::: {.column width=50%}

- Algorithm has two phases
  - Phase A takes time $O(g_{A}(n))$
  - Phase B takes time $O(g_{B}(n))$
- Algorithm as a whole takes time $\max(O(g_{A}(n),g_{B}(n)))$
- Least efficient phase is the upper bound for the whole algorithm

:::

::::

## Lower bounds

- $f(x)$ is said to be $\Omega(g(x))$ if we can find constants $c$ and $x_0$ such that $c \cdot g(x)$ is a lower bound for $f(x)$ for $x$ beyond $x_0$
  - $f(x) \geq cg(x)$ for every $x \geq x_0$
- $n^3$ is $\Omega(n^2)$
  - $n^3 > n^2$ for all $n$, so $n_0=1$, $c=1$
- Typically we establish lower bounds for a problem rather than an individual algorithm
  - If we sort a list by comparing elements and swapping them, we require $\Omega(n \log n)$ comparisons
  - This is *independent* of the sorting algorithm

## Tight bounds

- $f(x)$ is said to be $\Theta(g(x))$ if $f(x)$ is both $O(g(x))$ and $\Omega(g(x))$
  - Find constants $c_1$, $c_2$, $x_0$ such that $c_{1}g(x) \leq f(x) \leq c_{2}g(x)$ for every $x \geq x_0$
- $\frac{n(n-1)}{2}$ is $\Theta(n^2)$
  - Upper bound
    - $\frac{n(n-1)}{2} = \frac{n^2}{2} - \frac{n}{2} \leq \frac{n^2}{2}$ for all $n \geq 0$
  - Lower bound
    - $\frac{n(n-1)}{2} = \frac{n^2}{2} - \frac{n}{2} \geq \frac{n^2}{2} - (\frac{n}{2} \times \frac{n}{2}) \geq \frac{n^2}{4}$ for $n \geq 2$
  - Choose $n_0 = 2$, $c_1 = \frac{1}{4}$, $c_2 = \frac{1}{2}$
  
## Summary

- $f(n)$ is $O(g(n))$ means $f(n)$ is bounded above by $g(n)$
  - Useful to describe asymptotic worst case running time
- $f(n)$ is $\Omega(g(n))$ means $f(n)$ is bounded below by $g(n)$
  - Typically used for a problem as a whole, rather than an individual algorithm
- $f(n)$ is $\Theta(g(n))$ means $f(n)$ is bounded both above and below by $g(n)$
  - We have found an optimal algorithm for the problem